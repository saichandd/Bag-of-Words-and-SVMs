{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"colab":{"name":"Duppala_Sai_112684112_hw3.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"name":"HW3.ipynb","accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","ein.tags":"worksheet-0","id":"hVwJj1nUL4lx"},"source":["# CSE527 Homework 3\n","**Due date: 23:59 on Oct 22, 2019 (Thuesday)**\n","\n","In this semester, we will use Google Colab for the assignments, which allows us to utilize resources that some of us might not have in their local machines such as GPUs. You will need to use your Stony Brook (*.stonybrook.edu) account for coding and Google Drive to save your results.\n","\n","## Google Colab Tutorial\n","---\n","Go to https://colab.research.google.com/notebooks/, you will see a tutorial named \"Welcome to Colaboratory\" file, where you can learn the basics of using google colab.\n","\n","Settings used for assignments: ***Edit -> Notebook Settings -> Runtime Type (Python 3)***.\n","\n","\n","## Using SIFT in OpenCV 3.x.x in Colab\n","---\n","The default version of OpenCV in Colab is 3.4.3. If we use SIFT method directly, typically we will get this error message:\n","\n","```\n","error: OpenCV(3.4.3) /io/opencv_contrib/modules/xfeatures2d/src/sift.cpp:1207: error: (-213:The function/feature is not implemented) This algorithm is patented and is excluded in this configuration; Set OPENCV_ENABLE_NONFREE CMake option and rebuild the library in function 'create'\n","\n","```\n","\n","One simple way to use the OpenCV in-built function `SIFT` in Colab is to switch the version to the one from 'contrib'. Below is an example of switching OpenCV version:\n","\n","1. Run the following command in one section in Colab, which has already been included in this assignment:\n","```\n","pip install opencv-contrib-python==3.4.2.16\n","```\n","2. Restart runtime by\n","```\n","Runtime -> Restart Runtime\n","```\n","\n","Then you should be able to use use `cv2.xfeatures2d.SIFT_create()` to create a SIFT object, whose functions are listed at http://docs.opencv.org/3.0-beta/modules/xfeatures2d/doc/nonfree_features.html\n","\n","## Some Resources\n","---\n","In addition to the tutorial document, the following resources can definitely help you in this homework:\n","- http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_matcher/py_matcher.html\n","- http://docs.opencv.org/3.1.0/da/df5/tutorial_py_sift_intro.html\n","- http://docs.opencv.org/3.0-beta/modules/xfeatures2d/doc/nonfree_features.html?highlight=sift#cv2.SIFT\n","- http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_imgproc/py_geometric_transformations/py_geometric_transformations.html\n","\n","\n","## Description\n","---\n","In this homework, we will examine the task of scene recognition starting with\n","very simple methods: tiny images and nearest neighbor classification, and then\n","move on to more advanced methods: bags of quantized local features and linear\n","classifiers learned by support vector machines.\n","\n","Bag of words models are a popular technique for image classification inspired by\n","models used in natural language processing. The model ignores or downplays word\n","arrangement (spatial information in the image) and classifies based on a\n","histogram of the frequency of visual words. The visual word \"vocabulary\" is\n","established by clustering a large corpus of local features. See Szeliski chapter\n","14.4.1 for more details on category recognition with quantized features. In\n","addition, 14.3.2 discusses vocabulary creation and 14.1 covers classification\n","techniques.\n","\n","For this homework you will be implementing a basic bag of words model. You will\n","classify scenes into one of 16 categories by training and testing on the 16\n","scene database (introduced in [Lazebnik et al.\n","2006](http://www.di.ens.fr/willow/pdfs/cvpr06b.pdf), although built on top of\n","previously published datasets).\n","[Lazebnik et al. 2006](http://www.di.ens.fr/willow/pdfs/cvpr06b.pdf) is a great\n","paper to read, although we will be implementing the baseline method the paper\n","discusses (equivalent to the zero level pyramid) and not the more sophisticated\n","spatial pyramid. For an excellent survey of\n","pre-deep-learning feature encoding methods for bag of words models, see\n","[Chatfield et al, 2011](http://www.robots.ox.ac.uk/~vgg/research/encoding_eval/).\n","\n","You are required to implement 2 different image representations: tiny images and bags of SIFT features, and 2 different classification techniques: nearest neighbor and linear SVM. There are 3 problems plus a performance report in this homework with a total of 100 points. 1 bonus question with extra 10 points is provided under problem 3. The maximum points you may earn from this homework is 100 + 10 = 110 points. Be sure to read **Submission Guidelines** below. They are important.\n","\n","## Dataset\n","---\n","\n","The starter code trains on 150 and tests on 50 images from each category (i.e. 2400\n","training examples total and 800 test cases total). In a real research paper,\n","one would be expected to test performance on random splits of the data into\n","training and test sets, but the starter code does not do this to ease debugging.\n","\n","Save the [dataset(click me)](https://drive.google.com/drive/folders/1NWC3TMsXSWN2TeoYMCjhf2N1b-WRDh-M?usp=sharing) into your working folder in your Google Drive for this homework. <br>\n","Under your root folder, there should be a folder named \"data\" (i.e. XXX/Surname_Givenname_SBUID/data) containing the images.\n","**Do not upload** the data subfolder before submitting on blackboard due to size limit. There should be only one .ipynb file under your root folder Surname_Givenname_SBUID.\n","\n","\n","## Starter Code\n","---\n","To make your task a little easier, below we provide some starter code which\n","randomly guesses the category of every test image and achieves about 6.25% accuracy\n","(1 out of 16 guesses is correct)."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"k4dhRR_119C4","outputId":"46a0c7d4-6312-4ed5-8d9e-e4e13e757387","executionInfo":{"status":"ok","timestamp":1578525799980,"user_tz":300,"elapsed":12206,"user":{"displayName":"Sai Duppala","photoUrl":"","userId":"14598207617166329138"}},"colab":{"base_uri":"https://localhost:8080/","height":183}},"source":["pip install opencv-contrib-python==3.4.2.16"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting opencv-contrib-python==3.4.2.16\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/f1/66330f4042c4fb3b2d77a159db8e8916d9cdecc29bc8c1f56bc7f8a9bec9/opencv_contrib_python-3.4.2.16-cp36-cp36m-manylinux1_x86_64.whl (30.6MB)\n","\u001b[K     |████████████████████████████████| 30.6MB 102kB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-contrib-python==3.4.2.16) (1.17.5)\n","Installing collected packages: opencv-contrib-python\n","  Found existing installation: opencv-contrib-python 4.1.2.30\n","    Uninstalling opencv-contrib-python-4.1.2.30:\n","      Successfully uninstalled opencv-contrib-python-4.1.2.30\n","Successfully installed opencv-contrib-python-3.4.2.16\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TxrQ1z4sObHr","outputId":"cd5308a2-a94c-41e8-c6f7-0605ae5a02e7","executionInfo":{"status":"ok","timestamp":1578525804999,"user_tz":300,"elapsed":17212,"user":{"displayName":"Sai Duppala","photoUrl":"","userId":"14598207617166329138"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["# import packages here\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import glob\n","import itertools\n","import time\n","import zipfile\n","import torch\n","import torchvision\n","import gc\n","import pickle\n","from sklearn import svm\n","from skimage import color\n","from skimage import io\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.cluster import KMeans\n","from sklearn.neighbors import NearestNeighbors\n","from sklearn.metrics import accuracy_score\n","\n","print(cv2.__version__) # verify OpenCV version"],"execution_count":2,"outputs":[{"output_type":"stream","text":["3.4.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"VRU7ngb-2Jye","colab":{}},"source":["# Mount your google drive where you've saved your assignment folder\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LjAojc6I2QIa","colab":{}},"source":["# Set your working directory (in your google drive)\n","# Note that 'gdrive/My Drive/Y2019Fall/CSE-527-Intro-To-Computer-Vision/hw3' is just an example, \n","#   change it to your specific homework directory.\n","cd '/content/gdrive/My Drive/Duppala_Sai_112684112_hw3'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FhkgVLuQTHn7"},"source":["## Data Preparation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ENRccp9L2X1F","colab":{}},"source":["class_names = [name[13:] for name in glob.glob('./data/train/*')]\n","class_names = dict(zip(range(len(class_names)), class_names))\n","print(\"class_names: %s \" % class_names)\n","n_train_samples_per_class = 150\n","n_test_samples_per_class = 50\n","\n","def load_dataset(path, num_per_class=-1):\n","    data = []\n","    labels = []\n","    for id, class_name in class_names.items():\n","        print(\"Loading images from class: %s\" % id)\n","        img_path_class = glob.glob(path + class_name + '/*.jpg')\n","        if num_per_class > 0:\n","            img_path_class = img_path_class[:num_per_class]\n","        labels.extend([id]*len(img_path_class))\n","        for filename in img_path_class:\n","            data.append(cv2.imread(filename, 0))\n","    return data, labels\n","\n","# load training dataset\n","# train_data, train_label = load_dataset('./data/train/')\n","train_data, train_label = load_dataset('./data/train/', n_train_samples_per_class)\n","n_train = len(train_label)\n","print(\"n_train: %s\" % n_train)\n","\n","# load testing dataset\n","# test_data, test_label = load_dataset('./data/test/')\n","test_data, test_label = load_dataset('./data/test/', n_test_samples_per_class)\n","n_test = len(test_label)\n","print(\"n_test: %s\" % n_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"lVK5CH1V2cY3","colab":{}},"source":["# As loading the data from the source for the first time is time consuming\n","#so you can pkl or save the data into byte stream such that subsequent data loading is faster\n","# Save intermediate image data into disk\n","file = open('train.pkl','wb')\n","pickle.dump(train_data, file)\n","pickle.dump(train_label, file)\n","file.close()\n","\n","file = open('test.pkl','wb')\n","pickle.dump(test_data, file)\n","pickle.dump(test_label, file)\n","file.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sS51rm2O24lm","colab":{}},"source":["# Load intermediate image data from disk\n","file = open('train.pkl', 'rb')\n","train_data = pickle.load(file)\n","train_label = pickle.load(file)\n","file.close()\n","\n","file = open('test.pkl', 'rb')\n","test_data = pickle.load(file)\n","test_label = pickle.load(file)\n","file.close()\n","\n","# print(len(train_data), len(train_label)) # Verify number of training samples\n","# print(len(test_data), len(test_label))   # Verify number of testing samples"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","ein.tags":"worksheet-0","id":"iPF5y8C4L4mC"},"source":["## Problem 1: Tiny Image Representation + Nearest Neighbor Classifier\n","{25 points} You will start by implementing the tiny image representation and the nearest neighbor classifier. They are easy to understand, easy to implement, and run very quickly for our experimental setup.\n","\n","The \"tiny image\" feature is one of the simplest possible image representations. One simply resizes each image to a small, fixed resolution. You are required to **resize the image to 16x16**. It works slightly better if the tiny image is made to have zero mean and unit length (normalization). This is not a particularly good representation, because it discards all of the high frequency image content and is not especially invariant to spatial or brightness shifts. We are using tiny images simply as a baseline.\n","\n","The nearest neighbor classifier is equally simple to understand. When tasked with classifying a test feature into a particular category, one simply finds the \"nearest\" training example (L2 distance is a sufficient metric) and assigns the label of that nearest training example to the test example. The nearest neighbor classifier has many desirable features — it requires no training, it can learn arbitrarily complex decision boundaries, and it trivially supports multiclass problems. It is quite vulnerable to training noise, though, which can be alleviated by voting based on the K nearest neighbors (but you are not required to do so). Nearest neighbor classifiers also suffer as the feature dimensionality increases, because the classifier has no mechanism to learn which dimensions are irrelevant for the decision.\n","\n","Report your classification accuracy on the test sets and time consumption.\n","\n","**Hints**:\n","- Use [cv2.resize()](https://docs.opencv.org/2.4/modules/imgproc/doc/geometric_transformations.html#resize) to resize the images;\n","- Use [NearestNeighbors in Sklearn](http://scikit-learn.org/stable/modules/neighbors.html) as your nearest neighbor classifier."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JBTiTro73Q5E","colab":{}},"source":["# Write your codes here\n","\n","tiny_size = (16,16)\n","\n","#has its own pipeline, data is copied into it\n","train_data_tiny = list(map(lambda x: cv2.resize(x, tiny_size), train_data))\n","train_data_tiny = np.stack(train_data_tiny)\n","train_label = np.array(train_label)\n","test_data_tiny = list(map(lambda x: cv2.resize(x, tiny_size), test_data))\n","test_data_tiny = np.stack(test_data_tiny)\n","test_label = np.array(test_label)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"A-uhWlcaPrgx","colab_type":"code","colab":{}},"source":["\n","# feature extraction\n","def extract_features(raw_data):\n","    #using 256 features - which is maximum for 16*16 image\n","    featu_dim = 256\n","    featu_data = np.zeros((len(raw_data), featu_dim), dtype=np.float32)\n","    num_rows = featu_data.shape[0]\n","    for i in np.arange(num_rows):\n","        #Normalize\n","        norm_image = cv2.normalize(raw_data[i], None, alpha=-0.5, beta=0.5, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_64F)\n","        featu_data[i] = np.reshape(norm_image, (norm_image.size))[:featu_dim]\n","    # print(\"features\",len(featu_data))\n","\n","    return featu_data\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"spDUYqGINu-P","colab_type":"code","colab":{}},"source":["from sklearn.neighbors.nearest_centroid import NearestCentroid\n","\n","t11 = time.time()\n","#features of train and test data\n","train_feat = extract_features(train_data_tiny)\n","test_feat = extract_features(test_data_tiny)\n","\n","#Using nearest centroid\n","clf = NearestCentroid()\n","clf.fit(train_feat, train_label)\n","\n","label1 = test_label\n","#predictions\n","pred1 = clf.predict(test_feat)\n","\n","t12 = time.time()\n","#accuracy\n","accuracy1 = sum(np.array(pred1) == label1)*100 / float(n_test)\n","print(\"Accuracy(Tiny image) =\", accuracy1)\n","print(\"Time Taken = \", t12-t11)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","ein.tags":"worksheet-0","id":"bU8G7eLyL4mH"},"source":["## Problem 2: Bag of SIFT Representation + Nearest Neighbor Classifer\n","{35 points}\n","After you have implemented a baseline scene recognition pipeline it is time to\n","move on to a more sophisticated image representation — bags of quantized SIFT\n","features. Before we can represent our training and testing images as bag of\n","feature histograms, we first need to establish a vocabulary of visual words. We\n","will form this vocabulary by sampling many local features from our training set\n","(10's or 100's of thousands) and then cluster them with k-means. The number of\n","k-means clusters is the size of our vocabulary and the size of our features. For\n","example, you might start by clustering many SIFT descriptors into k=50 clusters.\n","This partitions the continuous, 128 dimensional SIFT feature space into 50\n","regions. For any new SIFT feature we observe, we can figure out which region it\n","belongs to as long as we save the centroids of our original clusters. Those\n","centroids are our visual word vocabulary. Because it can be slow to sample and\n","cluster many local features, the starter code saves the cluster centroids and\n","avoids recomputing them on future runs.\n","\n","Now we are ready to represent our training and testing images as histograms of\n","visual words. For each image we will densely sample many SIFT descriptors.\n","Instead of storing hundreds of SIFT descriptors, we simply count how many SIFT\n","descriptors fall into each cluster in our visual word vocabulary. This is done\n","by finding the nearest neighbor k-means centroid for every SIFT feature. Thus,\n","if we have a vocabulary of 50 visual words, and we detect 220 distinct SIFT\n","features in an image, our bag of SIFT representation will be a histogram of 50\n","dimensions where each bin counts how many times a SIFT descriptor was assigned\n","to that cluster. The total of all the bin-counts is 220. The histogram should be\n","normalized so that image size does not dramatically change the bag of features\n","magnitude.\n","\n","After you obtain the Bag of SIFT feature representation of the images, you have to\n","train a KNN classifier in the Bag of SIFT feature space and report your test set accuracy and time consumption. \n","\n","**Note**: \n","- Instead of using SIFT to detect invariant keypoints which is time-consuming,\n","  you are recommended to densely sample keypoints in a grid with certain step\n","  size (sampling density) and scale.\n","- There are many design decisions and free parameters for the bag of SIFT\n","  representation (number of clusters, sampling density, sampling scales, SIFT\n","  parameters, etc.) so accuracy might vary from 50% to 60%.\n","- Indicate clearly the parameters you use along with the prediction accuracy\n","  on test set and time consumption.\n","\n","**Hints**:\n","- Use [KMeans in Sklearn](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)\n","  to do clustering and find the nearest cluster centroid for each SIFT feature;\n","- Use `cv2.xfeatures2d.SIFT_create()` to create a SIFT object;\n","- Use `sift.compute()` to compute SIFT descriptors given densely sampled keypoints\n","  ([cv2.Keypoint](https://docs.opencv.org/3.0-beta/modules/core/doc/basic_structures.html?highlight=keypoint#keypoint)).\n","- Be mindful of RAM usage. Try to make the code more memory efficient, otherwise it could easily exceed RAM limits in Colab, at which point your session will crash.\n","- If your RAM is going to run out of space, use [gc.collect()](https://docs.python.org/3/library/gc.html) for the garbage collector to collect unused objects in  memory to free some space.\n","- Store data or features as NumPy arrays instead of lists. Computation on NumPy arrays is much more efficient than lists."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vu-tYYGz30L2","colab":{}},"source":["import cv2\n","\n","# defining SIFT, the feature extractor that we want to use\n","featureExtractor = cv2.xfeatures2d.SIFT_create()\n","\n","def features(image, extractor):\n","    return keypoints, descriptors\n","    keypoints, descriptors = extractor.detectAndCompute(image, None)\n","\n","#pass all descriptors vertically stacked and kmeans\n","def get_histogram(descriptors, cluster_algorithm):\n","    histogram = np.zeros(len(cluster_algorithm.cluster_centers_))\n","    cluster_result =  cluster_algorithm.predict(descriptors)\n","    for i in cluster_result:\n","        histogram[i] += 1.0\n","    return histogram\n","\n","def generate_vocab(images):\n","    temp_list = []\n","    for image in images:\n","        kp, descriptors = features(image, featureExtractor)\n","        temp_list.append(descriptors)\n","        \n","    #initilate with first descriptor\n","    vocab_list = temp_list[0]\n","    for e in temp_list:\n","        vocab_list = np.vstack((vocab_list, e))\n","        # print(vocab_list.shape)\n","    return (vocab_list, temp_list)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DW-DbK3TxX9U","colab_type":"code","colab":{}},"source":["#combine both train and test data to cluster\n","all_data = np.concatenate([train_data, test_data])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9wCsPuJ-wx9d","colab_type":"code","colab":{}},"source":["# kmeans on all_data\n","vocab_list, descriptors_list = generate_vocab(all_data)\n","\n","#number of clusters 50\n","kmeans = KMeans(n_clusters = 50)\n","kmeans.fit(vocab_list)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JKQddcpISWoO","colab_type":"code","colab":{}},"source":["from sklearn.cluster import KMeans\n","\n","train_histograms = []\n","for image in train_data:\n","        kp, descriptors = features(image, featureExtractor)\n","        histogram = get_histogram(descriptors, kmeans)\n","        train_histograms.append(histogram)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9JgreSx7NCPr","colab_type":"code","colab":{}},"source":["#test data\n","# vocab_list_test, descriptors_list_test = generate_vocab(test_data)\n","\n","test_histograms = []\n","for image in test_data:\n","    kp, descriptors = features(image, featureExtractor)\n","    histogram = get_histogram(descriptors, kmeans)\n","    test_histograms.append(histogram)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L1XwQGlhMZ8a","colab_type":"code","colab":{}},"source":["from sklearn.neighbors import KNeighborsClassifier\n","\n","model = KNeighborsClassifier(n_neighbors=5)\n","model.fit(train_histograms,train_label)\n","\n","# print(train_histograms)\n","label2 = test_label\n","pred2= model.predict(test_histograms)\n","accuracy2 = sum(np.array(pred2) == label2) / float(n_test)\n","\n","# print(predicted2)\n","print(accuracy2*100)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","ein.tags":"worksheet-0","id":"Z5cHPS2LL4mL"},"source":["## Problem 3.a: Bag of SIFT Representation + one-vs-all SVMs\n","{15 points}\n","The last task is to train one-vs-all linear SVMS to operate in the bag of SIFT\n","feature space. Linear classifiers are one of the simplest possible learning\n","models. The feature space is partitioned by a learned hyperplane and test cases\n","are categorized based on which side of that hyperplane they fall on. Despite\n","this model being far less expressive than the nearest neighbor classifier, it\n","will often perform better.\n","\n","You do not have to implement the support vector machine. However, linear\n","classifiers are inherently binary and we have a 16-way classification problem\n","(the library has handled it for you). To decide which of 16 categories a test\n","case belongs to, you will train 16 binary, one-vs-all SVMs. One-vs-all means\n","that each classifier will be trained to recognize 'forest' vs 'non-forest',\n","'kitchen' vs 'non-kitchen', etc. All 16 classifiers will be evaluated on each\n","test case and the classifier which is most confidently positive \"wins\". E.g. if\n","the 'kitchen' classifier returns a score of -0.2 (where 0 is on the decision\n","boundary), and the 'forest' classifier returns a score of -0.3, and all of the\n","other classifiers are even more negative, the test case would be classified as a\n","kitchen even though none of the classifiers put the test case on the positive\n","side of the decision boundary. When learning an SVM, you have a free parameter\n","$\\lambda$ (lambda) which controls how strongly regularized the model is. Your\n","accuracy will be very sensitive to $\\lambda$, so be sure to try many values.\n","\n","Indicate clearly the parameters you use along with the prediction accuracy on\n","test set and time consumption.\n","\n","**Bonus {10 points}**: For this question, you need to generate class prediction for the images in **test2** folder using your best model. The prediction file(**Surname_Givenname_SBUID_Pred.txt**) should follow the exact format as given in the **sample.txt** file.10 points will be given to students whose accuracy ranks top 3 in this homework.\n","\n","**Hints**:\n","- Use SVM in\n","  [Sklearn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm)\n","  (recommended) or\n","  [OpenCV](https://docs.opencv.org/3.0-alpha/modules/ml/doc/support_vector_machines.html)\n","  to do training and prediction."]},{"cell_type":"code","metadata":{"id":"vVywVEeTfXGW","colab_type":"code","colab":{}},"source":["from sklearn.multiclass import OneVsRestClassifier\n","from sklearn.svm import LinearSVC\n","from sklearn.utils.testing import ignore_warnings\n","from sklearn.exceptions import ConvergenceWarning\n","\n","@ignore_warnings(category=ConvergenceWarning)\n","def onevsrestModel(train_histograms, train_label):\n","    model3 = OneVsRestClassifier(LinearSVC( max_iter=100000))\n","    model3.fit(train_histograms, train_label)\n","\n","t31 = time.time()\n","onevsrestModel(train_histograms, train_label)\n","label3 =test_label\n","pred3 = model3.predict(test_histograms)\n","accuracy3 = sum(np.array(pred3) == label3) / float(n_test)\n","\n","t32 = time.time()\n","\n","print(\"Accuracy(one-vs-all SVMs) =\", accuracy3*100)\n","print(\"Time Taken = \", t32-t31)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"GPX6w0-GFGF6"},"source":["## Problem 3.b\n","{5 points} Repeat the evaluation above for different sizes of training sets and draw a plot to show how the size of the training set affects test performace. Do this for training set sizes of 800, 1200, 1600, 2000, 2200, and 2300 images. Randomly sample the images from the original training set and evaluate accuracy. Repeat this process 10 times for each training set size and report the average prediction accuracy. How does performance variability change with training set size? How does performance change? Give reason for your observations."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"kVpUDrMO3hwM","colab":{}},"source":["# Write your codes here\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","ein.tags":"worksheet-0","id":"6NtxxntZL4mP"},"source":["## Performance Report\n","---\n","{20 points}\n","Please report the performance of the following combinations **in the given order**\n","in terms of the time consumed and classification accuracy. Describe your algorithm,\n","any decisions you made to write your algorithm in your particular way, and how\n","different choices you made affect it. Compute and draw a (normalized) [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix), and discuss\n","where the method performs best and worse for each of the combination.\n","Here is an [example](http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py) of how to compute confusion matrix.\n","\n","\n","1st: Tiny images representation and nearest neighbor classifier (accuracy of about 18-25%).<br>\n","2nd: Bag of SIFT representation and nearest neighbor - classifier (accuracy of about 40-50%). <br>\n","3rd: Bag of SIFT representation and linear SVM classifier (accuracy of about 50-70%). <br>"]},{"cell_type":"markdown","metadata":{"id":"ML4QxaN-mr5t","colab_type":"text"},"source":["##Report\n","<ol>\n","<li>\n","<b>Tiny images representation and nearest neighbor classifier:</b><br>\n","Accuracy = 19.25%<br>\n","Time Consumed = 0.026 sec<br>\n","Algorithm: Made all the train and test images to size 16. Then extracted features after normalizing the images.\n","As the image is 16*16. I took 16*16 = 256 features considering all the pixels.\n","Not I predicted the test images using Nearest Neighbours from sklearn.\n","</li>\n","<li>\n","<b>Bag of SIFT representation and nearest neighbor - classifier:</b><br>\n","Accuracy = 48.5%<br>\n","Time Consumed = around 1hr 15min(took too long, so didn't run again)<br>\n","Algorithm: First I wrote a function to get the vocabulary by sampling local features from our training set. Now I've clustered them with k-means into 50 clusters. The main reason the algoritm took very long is because I used SIFT to detect invariant keypoints which is time-consuming. This is the reason why I didn't computer time.<br>\n","After this I've obtained features, whicih are histograms for train and test. And I've fitted the training data to KNeighborsClassifier with n_neighbours as 5 which is the deafult option.\n","</li>\n","<li>\n","<b>Bag of SIFT representation and linear SVM classifier:</b><br>\n","Accuracy = 52.25%<br>\n","Time Consumed = 283.8 sec<br>\n","Algorithm: Using the same features from the previous problem I've trained the OVR classifiers with linear svm. I've tried variours max_iteration and after a certain point the accuracy seemed to plateu.\n","</li>\n","</ol>\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-EBU10i84d7X","colab":{}},"source":["from sklearn.metrics import confusion_matrix\n","\n","def plot_confusion_matrix(y_true, y_pred, classes,\n","                          normalize=True,\n","                          title='Confusion matrix',\n","                          cmap=plt.cm.Blues):\n","\n","    # Compute confusion matrix\n","    # Only use the labels that appear in the data\n","    # classes = classes[unique_labels(y_true, y_pred)]\n","\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","    \n","    fig, ax = plt.subplots(figsize=(12,12))\n","    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n","    ax.figure.colorbar(im, ax=ax)\n","    # We want to show all ticks...\n","    ax.set(xticks=np.arange(cm.shape[1]),\n","           yticks=np.arange(cm.shape[0]),\n","           # ... and label them with the respective list entries\n","           xticklabels=classes, yticklabels=classes,\n","           title=title,\n","           ylabel='True label',\n","           xlabel='Predicted label')\n","\n","    # Rotate the tick labels and set their alignment.\n","    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n","             rotation_mode=\"anchor\")\n","\n","    # Loop over data dimensions and create text annotations.\n","    fmt = '.2f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            ax.text(j, i, format(cm[i, j], fmt),\n","                    ha=\"center\", va=\"center\",\n","                    color=\"white\" if cm[i, j] > thresh else \"black\")\n","    fig.tight_layout()\n","    return ax\n","\n","c_names = [name[13:] for name in glob.glob('./data/train/*')]\n","\n","#First combination:\n","# Confusion matrix\n","plot_confusion_matrix(label1, pred1, c_names, normalize=True)\n","plt.show()\n","\n","#Second combination:\n","# Confusion matrix\n","plot_confusion_matrix(label2, pred2, c_names, normalize=True)\n","plt.show()\n","\n","#Third combination:\n","# Confusion matrix\n","plot_confusion_matrix(label2, pred2, c_names, normalize=True)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"PhZehMVdL4mS"},"source":["## Submission guidelines\n","---\n","Extract the downloaded .zip file to a folder of your preference. The input and output paths are predefined and **DO NOT** change them, (we assume that 'Surname_Givenname_SBUID_hw3' is your working directory, and all the paths are relative to this directory).  The image read and write functions are already written for you. All you need to do is to fill in the blanks as indicated to generate proper outputs. **DO NOT** zip and upload the dataset on blackboard due to size limit.\n","\n","When submitting your .zip file through blackboard, please\n","-- name your .zip file as **Surname_Givenname_SBUID_hw*.zip**.\n","\n","This zip file should include:\n","```\n","Surname_Givenname_SBUID_hw*\n","        |---Surname_Givenname_SBUID_hw*.ipynb\n","        |---Surname_Givenname_SBUID_hw*.pdf\n","        |---Surname_Givenname_SBUID_Pred*.txt\n","```\n","\n","For instance, student Michael Jordan should submit a zip file named \"Jordan_Michael_111134567_hw3.zip\" for homework3 in this structure:\n","```\n","Jordan_Michael_111134567_hw3\n","        |---Jordan_Michael_111134567_hw3.ipynb\n","        |---Jordan_Michael_111134567_hw3.pdf\n","        |---Jordan_Michael_111134567_Pred.txt\n","```\n","\n","The **Surname_Givenname_SBUID_hw*.pdf** should include a **google shared link** and **Surname_Givenname_SBUID_Pred*.pdf** should be your test set prediction file in the specified format. To generate the **google shared link**, first create a folder named **Surname_Givenname_SBUID_hw*** in your Google Drive with your Stony Brook account. The structure of the files in the folder should be exactly the same as the one you downloaded. If you alter the folder structures, the grading of your homework will be significantly delayed and possibly penalized.\n","\n","Then right click this folder, click ***Get shareable link***, in the People textfield, enter two TA's emails: ***bo.cao.1@stonybrook.edu*** and ***sayontan.ghosh@stonybrook.edu***. Make sure that TAs who have the link **can edit**, ***not just*** **can view**, and also **uncheck** the **Notify people** box.\n","\n","Colab has a good feature of version control, you should take advantage of this to save your work properly. However, the timestamp of the submission made in blackboard is the only one that we consider for grading. To be more specific, we will only grade the version of your code right before the timestamp of the submission made in blackboard. \n","\n","You are encouraged to post and answer questions on Piazza. Based on the amount of email that we have received in past years, there might be dealys in replying to personal emails. Please ask questions on Piazza and send emails only for personal issues.\n","\n","Be aware that your code will undergo plagiarism check both vertically and horizontally. Please do your own work.\n","\n","**Late submission penalty:** <br>\n","There will be a 10% penalty per day for late submission. However, you will have 4 days throughout the whole semester to submit late without penalty. Note that the grace period is calculated by days instead of hours. If you submit the homework one minute after the deadline, one late day will be counted. Likewise, if you submit one minute after the deadline, the 10% penaly will be imposed if not using the grace period.\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"nio-kNUh947x","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}